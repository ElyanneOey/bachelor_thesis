{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: larq in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: packaging>=19.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from larq) (23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.15.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from larq) (1.16.4)\n",
      "Requirement already satisfied: importlib-metadata<4.0,>=2.0; python_version < \"3.8\" in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from larq) (3.10.1)\n",
      "Requirement already satisfied: terminaltables>=3.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from larq) (3.1.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0,>=2.0; python_version < \"3.8\"->larq) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0,>=2.0; python_version < \"3.8\"->larq) (4.0.1)\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-tuner) (2.22.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2022.12.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install larq\n",
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "import larq as lq\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_left_under = 50000\n",
    "num_images_right_above = 50000\n",
    "\n",
    "dataset_left_under = np.empty((num_images_left_under, 2, 2), dtype=np.uint8)\n",
    "dataset_right_above = np.empty((num_images_right_above, 2, 2), dtype=np.uint8)\n",
    "\n",
    "for i in range(num_images_left_under):\n",
    "    image = np.zeros((2, 2), dtype=np.uint8)\n",
    "    image[0, 0] = np.random.randint(0, 255) #255 not included\n",
    "    image[0, 1] = np.random.randint(0, 255)\n",
    "    image[1, 1] = np.random.randint(0, 255)\n",
    "    image[1, 0] = 255\n",
    "    dataset_left_under[i] = image\n",
    "    \n",
    "for i in range(num_images_right_above):\n",
    "    image = np.zeros((2, 2), dtype=np.uint8)\n",
    "    image[0, 0] = np.random.randint(0, 255)\n",
    "    image[1, 0] = np.random.randint(0, 255)\n",
    "    image[1, 1] = np.random.randint(0, 255)\n",
    "    image[0, 1] = 255\n",
    "    dataset_right_above[i] = image    \n",
    "\n",
    "y_left_under = np.full(num_images_left_under, 1)\n",
    "y_right_above = np.full(num_images_right_above, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure\n",
    "plt.imshow(dataset_left_under[10], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset = np.concatenate((dataset_left_under, dataset_right_above))\n",
    "y_dataset = np.concatenate((y_left_under, y_right_above))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_dataset, y_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape(80000, 4).astype(\"float32\") / 127.5 - 1, x_test.reshape(20000, 4).astype(\"float32\") / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset resize to 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "12665\n",
      "2115\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist # Dataset consistent of 28x28 images of the handwritten numbers 0 to 9\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "\n",
    "print(x_train.shape) \n",
    "#print(x_train)\n",
    "train_0_1 = np.where((y_train == 0 ) | (y_train == 1))\n",
    "test_0_1 = np.where((y_test == 0) | (y_test == 1))\n",
    "x_train, y_train = x_train[train_0_1], y_train[train_0_1]\n",
    "x_test, y_test = x_test[test_0_1], y_test[test_0_1]\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize\n",
    "x_train = np.expand_dims(x_train, axis=-1) #extra dimension for resizing\n",
    "x_train = tf.image.resize(x_train, [2,2]) \n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "x_test = tf.image.resize(x_test, [2,2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extra dimension\n",
    "x_train = x_train[:, :, :, 0] \n",
    "x_test = x_test[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow to numpy array\n",
    "x_train = x_train.numpy()\n",
    "x_test = x_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure\n",
    "plt.imshow(x_train[10], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape(12665, 4).astype(\"float32\") / 127.5 - 1, x_test.reshape(2115, 4).astype(\"float32\") / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! don't use this one\n",
    "\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# In the first layer we only quantize the weights and not the input\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(lq.layers.QuantDense(2,\n",
    "                                activation='relu',\n",
    "                                kernel_quantizer=\"ste_sign\",\n",
    "                                kernel_constraint=\"weight_clip\",\n",
    "                                use_bias=False,\n",
    "                                input_shape=(2, 2)))\n",
    "#model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model.add(lq.layers.QuantDense(2, activation='softmax', use_bias=False, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! use this one\n",
    "def create_model():\n",
    "    kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "                  kernel_quantizer=\"ste_sign\",\n",
    "                  kernel_constraint=\"weight_clip\")\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # In the first layer we only quantize the weights and not the input\n",
    "    #model.add(tf.keras.layers.Flatten(input_shape=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(lq.layers.QuantDense(6,\n",
    "                                    kernel_quantizer=\"ste_sign\",\n",
    "                                    kernel_constraint=\"weight_clip\",\n",
    "                                    use_bias=False))\n",
    "\n",
    "    model.add(lq.layers.QuantDense(2,\n",
    "                                    input_quantizer=\"ste_sign\",\n",
    "                                    kernel_quantizer=\"ste_sign\",\n",
    "                                    kernel_constraint=\"weight_clip\",\n",
    "                                    use_bias=False))\n",
    "    #model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    #model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "    #model.add(lq.layers.QuantDense(2, activation='softmax', use_bias=False, **kwargs))\n",
    "    model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 2s 6ms/step - loss: 0.4541 - accuracy: 0.8364\n",
      "67/67 [==============================] - 1s 1ms/step - loss: 0.9693 - accuracy: 0.4506\n",
      "67/67 [==============================] - 1s 1ms/step - loss: 0.6716 - accuracy: 0.7277\n",
      "67/67 [==============================] - 0s 756us/step - loss: 0.7113 - accuracy: 0.7078\n",
      "67/67 [==============================] - 1s 750us/step - loss: 0.9368 - accuracy: 0.3920\n",
      "67/67 [==============================] - 0s 848us/step - loss: 0.6735 - accuracy: 0.7267\n",
      "67/67 [==============================] - 0s 747us/step - loss: 0.5742 - accuracy: 0.4615\n",
      "67/67 [==============================] - 0s 771us/step - loss: 0.6296 - accuracy: 0.8619\n",
      "67/67 [==============================] - 0s 798us/step - loss: 0.6735 - accuracy: 0.7267\n",
      "67/67 [==============================] - 0s 741us/step - loss: 0.6837 - accuracy: 0.4449\n"
     ]
    }
   ],
   "source": [
    "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',\n",
    "#             metrics=['accuracy'])\n",
    "\n",
    "accu = []\n",
    "\n",
    "for i in range(10):\n",
    "    model = create_model()\n",
    "    # When the next line is executed, the training of the model is done.\n",
    "    # Epochs stands for the number of cycles the ANN trains for, with the entire training dataset. \n",
    "    #history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=100)\n",
    "    model.fit(x_train, y_train, epochs=50, batch_size=75, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    accu.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 796us/step - loss: 0.4927 - accuracy: 0.8331\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.00000000e+00,  7.96708763e-01,  2.17636526e-01,\n",
      "        -1.82388812e-01,  8.26059058e-02,  8.94632578e-01],\n",
      "       [ 1.33934757e-03, -7.07818493e-02,  2.19957098e-01,\n",
      "         3.27603018e-04, -6.58553600e-01, -2.28876635e-01],\n",
      "       [-3.02159823e-02, -3.54662712e-04, -4.03533041e-01,\n",
      "         6.17746592e-01, -7.28839457e-01,  9.22197938e-01],\n",
      "       [ 6.05950714e-04, -1.08276166e-01,  9.81539270e-05,\n",
      "         8.09327245e-01,  2.79122778e-05, -2.84339249e-01]], dtype=float32), array([[-1.3782805e-02, -3.7056890e-03],\n",
      "       [-7.5385942e-05,  9.7954434e-01],\n",
      "       [-7.7918380e-01,  1.7418148e-04],\n",
      "       [ 1.0257229e-04, -3.5972688e-01],\n",
      "       [-5.5621982e-05,  2.9301235e-01],\n",
      "       [-4.7416243e-01, -5.2719682e-01]], dtype=float32)]\n",
      "[array([[ 1.,  1.,  1., -1.,  1.,  1.],\n",
      "       [ 1., -1.,  1.,  1., -1., -1.],\n",
      "       [-1., -1., -1.,  1., -1.,  1.],\n",
      "       [ 1., -1.,  1.,  1.,  1., -1.]], dtype=float32), array([[-1., -1.],\n",
      "       [-1.,  1.],\n",
      "       [-1.,  1.],\n",
      "       [ 1., -1.],\n",
      "       [-1.,  1.],\n",
      "       [-1., -1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.save(\"full_precision_model.h5\")  # save full precision latent weights\n",
    "fp_weights = model.get_weights()  # get latent weights\n",
    "print(fp_weights)\n",
    "with lq.context.quantized_scope(True):\n",
    "    model.save(\"binary_model.h5\")  # save binary weights\n",
    "    weights = model.get_weights()  # get binary weights\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.6336170256137847\n",
      "Standard Deviation: 0.1679508866279757\n"
     ]
    }
   ],
   "source": [
    "data = [0.7277, 0.7277, 0.8217]  # Replace with your data\n",
    "\n",
    "mean = np.mean(accu)\n",
    "std = np.std(accu)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
